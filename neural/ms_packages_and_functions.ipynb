{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ms packages and functions\n",
    "\n",
    "repository of all important packages and functions for network analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "<frozen importlib._bootstrap>:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "/home/mjms/code/umap/umap/__init__.py:9: UserWarning: Tensorflow not installed; ParametricUMAP will be unavailable\n",
      "  warn(\"Tensorflow not installed; ParametricUMAP will be unavailable\")\n",
      "<frozen importlib._bootstrap>:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "<frozen importlib._bootstrap>:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "/home/mjms/anaconda3/envs/reach-analysis/lib/python3.8/site-packages/bids/layout/validation.py:46: UserWarning: The ability to pass arguments to BIDSLayout that control indexing is likely to be removed in future; possibly as early as PyBIDS 0.14. This includes the `config_filename`, `ignore`, `force_index`, and `index_metadata` arguments. The recommended usage pattern is to initialize a new BIDSLayoutIndexer with these arguments, and pass it to the BIDSLayout via the `indexer` argument.\n",
      "  warnings.warn(\"The ability to pass arguments to BIDSLayout that control \"\n"
     ]
    }
   ],
   "source": [
    "#########################\n",
    "# IMPORTING PACKAGES\n",
    "#########################\n",
    "\n",
    "\n",
    "# FOR VISUALIZING\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "from astropy.convolution import Gaussian1DKernel\n",
    "from astropy.convolution import convolve\n",
    "import seaborn as sns\n",
    "from matplotlib import animation, rc\n",
    "from IPython.display import HTML\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['animation.embed_limit'] = 2**128\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from IPython.display import display, clear_output\n",
    "# import plotly.express as px\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "from matplotlib.lines import Line2D\n",
    "%matplotlib inline\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "mpl.rc('font',family='Roboto')\n",
    "from statannotations.Annotator import Annotator\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "from matplotlib.patches import Rectangle\n",
    "import string\n",
    "\n",
    "# FOR COMPUTATION\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "from scipy.spatial import distance as dist\n",
    "import math\n",
    "from collections import Counter\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "import sklearn.datasets\n",
    "import random\n",
    "from scipy import stats\n",
    "import itertools\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import umap\n",
    "import umap.plot\n",
    "import umap.utils as utils\n",
    "import umap.aligned_umap\n",
    "from scipy import interpolate\n",
    "import hdbscan\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "import neo\n",
    "import elephant as el\n",
    "import quantities as pq\n",
    "import re\n",
    "from multiprocess import Pool\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "# FOR SAVING/DATA HANDLING\n",
    "import sys\n",
    "from os.path import dirname, join as pjoin\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import time\n",
    "import copy\n",
    "import datetime\n",
    "import h5py\n",
    "import mat73\n",
    "from datetime import date\n",
    "\n",
    "# FOR NETWORK ANALYSIS\n",
    "import networkx as nx\n",
    "from nodevectors import Node2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "# from node2vec import Node2Vec #original node2vec implementation\n",
    "from teneto import TemporalNetwork\n",
    "import teneto\n",
    "import infomap\n",
    "import community\n",
    "#import igraph as ig\n",
    "import bct\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FOR DATA WRANGLING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bin_spikes(raster_data, startTime, endTime, binwin, binary=True, plot = True, cmap = 'binary_r', gauss = False, std = 0.1):\n",
    "\n",
    "|Input | Process| Output\n",
    "|--- | ---|---|\n",
    "|a trial from <br> `trials.json`| bins spike times | `binned_spks`<br>`bin_edges`<br>optional:plots a raster plot|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_spikes(raster_data, startTime, endTime, binwin, binary=True, plot = True, cmap = 'binary_r', gauss = False, std = 0.1):\n",
    "    # required packages: numpy, astropy (if convolving), matplotlib (if plotting)\n",
    "    \n",
    "    binned_spks = []\n",
    "\n",
    "    if gauss:\n",
    "        kernel = Gaussian1DKernel(std)\n",
    "        \n",
    "    for n, neuron in enumerate(raster_data['spiketimes']):\n",
    "        bins = np.arange(startTime,endTime,binwin)\n",
    "        binned, bin_edges = np.histogram(neuron,bins)\n",
    "\n",
    "        if binary:\n",
    "            binned[binned>0] = 1\n",
    "\n",
    "        if gauss:\n",
    "            binned = convolve(binned,kernel)\n",
    "            \n",
    "        binned_spks.append(binned)\n",
    "        \n",
    "    if plot:\n",
    "        plt.imshow(binned_spks,cmap=cmap)\n",
    "        if gauss:\n",
    "            plt.colorbar()\n",
    "    \n",
    "    return binned_spks, bin_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FOR FUNCTIONAL NETWORK CONSTRUCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MI(prob_matrix):\n",
    "\n",
    "    # input: prob_matrix of shape (s,r)\n",
    "    # stimulus: presyn: rows\n",
    "    # (number of stimulus = rows in matrix = prob_matrix.shape[0])\n",
    "    # response: postsyn: columns\n",
    "    # (number of stimulus = columns in matrix = prob_matrix.shape[0])\n",
    "\n",
    "    MI = 0\n",
    "    for r, response in enumerate(np.sum(prob_matrix,axis=1)):\n",
    "        for s, stimulus in enumerate(np.sum(prob_matrix,axis=0)):\n",
    "            \n",
    "            pr = response \n",
    "            ps = stimulus\n",
    "            \n",
    "            if (prob_matrix[r,s] != 0) and (ps!=0) and (pr!=0):\n",
    "                MI += prob_matrix[r,s]*(np.log2(prob_matrix[r,s]) - np.log2(pr*ps))\n",
    "    \n",
    "    return MI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_FN(FN_data,metric='fMI',plot=False,self_edge=False, norm=False):\n",
    "    \n",
    "    FN = np.zeros((len(FN_data),len(FN_data)))\n",
    "    \n",
    "    # Mutual Information\n",
    "    if metric=='MI':\n",
    "        for i, presyn in enumerate(FN_data):\n",
    "            for j, postsyn in enumerate(FN_data):\n",
    "                prob_matrix = np.zeros((2,2))\n",
    "\n",
    "                # Both are active\n",
    "                aa = np.where(np.logical_and(presyn>0,postsyn>0))\n",
    "                aa = len(aa[0].tolist())/len(presyn) \n",
    "                prob_matrix[1,1] = aa\n",
    "\n",
    "                # Both are inactive\n",
    "                ii =  np.where(np.logical_and(presyn==0,postsyn==0))\n",
    "                ii = len(ii[0].tolist())/len(presyn)\n",
    "                prob_matrix[0,0] = ii\n",
    "\n",
    "                # Only presyn is active\n",
    "                ai =  np.where(np.logical_and(presyn>0,postsyn==0))\n",
    "                ai = len(ai[0].tolist())/len(presyn)\n",
    "                prob_matrix[1,0] = ai\n",
    "\n",
    "                # only postsyn is active\n",
    "                ia =  np.where(np.logical_and(presyn==0,postsyn>0))\n",
    "                ia = len(ia[0].tolist())/len(presyn)\n",
    "                prob_matrix[0,1] = ia\n",
    "\n",
    "                if norm:\n",
    "                    FN[i,j] = normalized_mutual_info_score(presyn,postsyn)\n",
    "                else:\n",
    "                    MIij = MI(prob_matrix)\n",
    "                    FN[i,j] = MIij\n",
    "                \n",
    "    # Consecutive Mutual Information\n",
    "    elif metric=='cMI':\n",
    "        for i, presyn in enumerate(FN_data):\n",
    "            presyn  = presyn[:-1] #don't include last (to keep everything in the correct order)\n",
    "            for j, postsyn in enumerate(FN_data):\n",
    "                # shift one timebin over\n",
    "                postsyn = postsyn[1:] #don't include the first (because that is rolled value)\n",
    "                \n",
    "                prob_matrix = np.zeros((2,2))\n",
    "\n",
    "                # Both are active\n",
    "                aa = np.where(np.logical_and(presyn>0,postsyn>0))\n",
    "                aa = len(aa[0].tolist())/len(presyn) \n",
    "                prob_matrix[1,1] = aa\n",
    "\n",
    "                # Both are inactive\n",
    "                ii =  np.where(np.logical_and(presyn==0,postsyn==0))\n",
    "                ii = len(ii[0].tolist())/len(presyn)\n",
    "                prob_matrix[0,0] = ii\n",
    "\n",
    "                # Only presyn is active\n",
    "                ai =  np.where(np.logical_and(presyn>0,postsyn==0))\n",
    "                ai = len(ai[0].tolist())/len(presyn)\n",
    "                prob_matrix[1,0] = ai\n",
    "\n",
    "                # only postsyn is active\n",
    "                ia =  np.where(np.logical_and(presyn==0,postsyn>0))\n",
    "                ia = len(ia[0].tolist())/len(presyn)\n",
    "                prob_matrix[0,1] = ia\n",
    "\n",
    "                if norm:\n",
    "                    FN[i,j] = normalized_mutual_info_score(presyn,postsyn)\n",
    "                else:\n",
    "                    MIij = MI(prob_matrix)\n",
    "                    FN[i,j] = MIij\n",
    "                \n",
    "    #full Mutual Information\n",
    "    elif metric=='fMI':\n",
    "        for i, presyn in enumerate(FN_data):\n",
    "            presyn  = presyn[:-1] #don't include last (to keep everything in the correct order)\n",
    "            for j, postsyn in enumerate(FN_data):\n",
    "                # shift one timebin over\n",
    "                postsyn = postsyn[1:] + postsyn[:-1] # look at both consecutive and simoulaneous timebins!\n",
    "                postsyn[postsyn>0] = 1\n",
    "            \n",
    "                prob_matrix = np.zeros((2,2))\n",
    "\n",
    "                # Both are active\n",
    "                aa = np.where(np.logical_and(presyn>0,postsyn>0))\n",
    "                aa = len(aa[0].tolist())/len(presyn) \n",
    "                prob_matrix[1,1] = aa\n",
    "\n",
    "                # Both are inactive\n",
    "                ii =  np.where(np.logical_and(presyn==0,postsyn==0))\n",
    "                ii = len(ii[0].tolist())/len(presyn)\n",
    "                prob_matrix[0,0] = ii\n",
    "\n",
    "                # Only presyn is active\n",
    "                ai =  np.where(np.logical_and(presyn>0,postsyn==0))\n",
    "                ai = len(ai[0].tolist())/len(presyn)\n",
    "                prob_matrix[1,0] = ai\n",
    "\n",
    "                # only postsyn is active\n",
    "                ia =  np.where(np.logical_and(presyn==0,postsyn>0))\n",
    "                ia = len(ia[0].tolist())/len(presyn)\n",
    "                prob_matrix[0,1] = ia\n",
    "\n",
    "                if norm:\n",
    "                    FN[i,j] = normalized_mutual_info_score(presyn,postsyn)\n",
    "                else:\n",
    "                    MIij = MI(prob_matrix)\n",
    "                    FN[i,j] = MIij   \n",
    "    #elif metric=='corr':\n",
    "         \n",
    "    #elif metric=='lagcorr':\n",
    "    else:\n",
    "        print('That metric does not exist!')\n",
    "     \n",
    "    if not self_edge: #zero out diagonal\n",
    "        np.fill_diagonal(FN,0)\n",
    "        \n",
    "    if plot:\n",
    "        sns.heatmap(FN,cmap= 'magma',square=True)\n",
    "        plt.show()\n",
    "        \n",
    "    return FN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FOR FUNCTIONAL NETWORK ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mat2edgelist(matrix,weighted=False,weight_threshold=0):\n",
    "    # INPUT:\n",
    "    # matrix = adjecency matrix\n",
    "    # OUTPUT:\n",
    "    # edgelist = formatted as a list [[source],[target],[weight]]\n",
    "    \n",
    "    edgelist = []\n",
    "    for i,ival in enumerate(matrix):\n",
    "        for j, jval in enumerate(ival):\n",
    "            if not weighted:\n",
    "                weight = 1*(jval>weight_threshold)\n",
    "            else:\n",
    "                weight = jval\n",
    "            if weight>=weight_threshold:\n",
    "                edgelist.append([i,j,jval])\n",
    "    return edgelist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findInfomapCommunities(G, N=10):\n",
    "\n",
    "    infomapWrapper = infomap.Infomap(\"--directed --two-level -0 -N{}\".format(N))\n",
    "\n",
    "    for e in G.edges():\n",
    "        infomapWrapper.addLink(*e)\n",
    "\n",
    "    infomapWrapper.run();\n",
    "    \n",
    "    communities = infomapWrapper.getModules()\n",
    "    \n",
    "    nx.set_node_attributes(G, communities, 'community')\n",
    "    \n",
    "    return infomapWrapper.numTopModules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findLouvainCommunities(G, N=10):\n",
    "    \"\"\"\n",
    "    Partition a networkx graph G with the Louvain algorithm.\n",
    "    Annotates nodes with 'community' id and returns the number of communities found.\n",
    "    \"\"\"\n",
    "    best_partition = None\n",
    "    best_modularity = float(\"inf\")\n",
    "    for i in range(N):\n",
    "        partition = community.best_partition(G, randomize=True)\n",
    "        if community.modularity(partition, G) < best_modularity:\n",
    "            best_partition = partition\n",
    "    nx.set_node_attributes(G, name='community', values=best_partition)\n",
    "    return len(Counter(best_partition.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNetwork(graph_data,threshold):\n",
    "    edgelist = mat2edgelist(graph_data,weighted=True,weight_threshold=threshold)\n",
    "    g = nx.DiGraph()\n",
    "    for i in edgelist:\n",
    "        g.add_edge(i[0],i[1],weight=i[2])\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poissSpkTrain_r(T,dt,tau,r0):\n",
    "    num_steps = T/dt #number of iterations\n",
    "    t_steps = np.arange(0,T,dt)\n",
    "\n",
    "    spkTimes = []\n",
    "    isi = []\n",
    "    spkTrain = []\n",
    "\n",
    "    t_old = dt\n",
    "    r_old = r0\n",
    "    r     = r0\n",
    "    for i in t_steps:\n",
    "        tmp = random.random()\n",
    "        pf = r * dt\n",
    "\n",
    "        if tmp <= pf:\n",
    "            spkTimes.append(i)\n",
    "            isi.append(i-t_old)\n",
    "            t_old = i\n",
    "\n",
    "            r = 0\n",
    "\n",
    "            spkTrain.append(1)\n",
    "        else:\n",
    "            spkTrain.append(0)\n",
    "\n",
    "        r_old = r\n",
    "        dr = (r0-r_old)*(dt/tau)\n",
    "        r  = r+dr\n",
    "    \n",
    "    return spkTimes,isi,spkTrain,t_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poissSpkTrain(r_est,T,dt):\n",
    "    '''\n",
    "    INPUT:\n",
    "    r_est = estimated firing rate (spks/s)\n",
    "    dt    = time steps (s)\n",
    "    T     = total time (s)\n",
    "    '''\n",
    "    pf = r_est * dt #probability of firing\n",
    "\n",
    "    num_steps = T/dt #number of iterations\n",
    "    t_steps = np.arange(0,T,dt)\n",
    "\n",
    "    spkTimes = []\n",
    "    isi = []\n",
    "    spkTrain = []\n",
    "\n",
    "    t_old = dt\n",
    "    \n",
    "    for i in t_steps:\n",
    "        tmp = random.random()\n",
    "        if tmp <= pf:\n",
    "            spkTimes.append(i)\n",
    "            isi.append(i-t_old)\n",
    "            t_old = i\n",
    "            \n",
    "            spkTrain.append(1)\n",
    "        else:\n",
    "            spkTrain.append(0)\n",
    "\n",
    "    return spkTimes,isi,spkTrain,t_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from : https://stackoverflow.com/questions/2566412/find-nearest-value-in-numpy-array \n",
    "def find_nearest(array, value):\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return array[idx], idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def status(string):\n",
    "    clear_output(wait=True)                \n",
    "    return display(string) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Graph Alignment**\n",
    "\n",
    "From Maayan Levy (a function for Graph Alignment in Matlab):\n",
    "```matlab\n",
    "function align_score = lp_alignment(adj1, adj2)\n",
    "    mp = cat(3, adj1, adj2);\n",
    "    min_mat = min(mp,[],3);\n",
    "    align_score = (2*sum(sum(min_mat)))/(sum(sum(sum(mp))));\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_score(G1,G2):\n",
    "    # todo error if G1 and G2 are not the same dimension\n",
    "    min_mat = np.minimum(G1,G2)\n",
    "    align_score = (2*np.sum(min_mat))/np.sum([np.sum(G1),np.sum(G2)])\n",
    "    return align_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_align_score_mp(G):\n",
    "    G1 = G[0]\n",
    "    G2 = G[1]\n",
    "    c  = G[2] # circ distance\n",
    "    \n",
    "    return [c,align_score(G1,G2)]\n",
    "    # null_alignment_scores = []\n",
    "\n",
    "    # min_trial_length = min([len(z[0]) for y in null_FNs[0] for x in y for z in x])\n",
    "\n",
    "    # all_trials = [[dir,trial] for dir in range(0,len(null_FNs[0]))\n",
    "    #                         for trial in range(0,len(null_FNs[0][dir][0]))]\n",
    "\n",
    "    # FN_pairs = [y for y in itertools.combinations(all_trials,2)]\n",
    "\n",
    "    # cdist = [circdistance(8,y[0][0],y[1][0]) for y in FN_pairs]\n",
    "\n",
    "    # for t in range(0,min_trial_length):\n",
    "    #     status(\"getting alignment scores for time: {}s\".format(bin2time(t)))       \n",
    "    #     null_alignment_scores.append([])\n",
    "    #     [null_alignment_scores[t].append([]) for x in np.unique(cdist)]\n",
    "\n",
    "    #     for c, pair in zip(cdist,FN_pairs):\n",
    "    #         score = align_score(null_FNs[0][pair[0][0]][0][pair[0][1]][0][time],null_FNs[0][pair[1][0]][0][pair[1][1]][0][time])\n",
    "    #         null_alignment_scores[t][c].append(score)\n",
    "\n",
    "    # return null_alignment_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meanFRoverSims(mp_data):\n",
    "    data_fr = mp_data[0]\n",
    "    trial_shape = mp_data[1]\n",
    "\n",
    "    binwin = 0.01\n",
    "    mov_window = 0.2 # 200 ms\n",
    "    mov_10ms = int(0.01/binwin) # move the mov_window this many bins\n",
    "    windowBin = int(mov_window/binwin)\n",
    "\n",
    "\n",
    "    poiss_spiketrain = el.spike_train_generation.inhomogeneous_poisson_process(data_fr)\n",
    "\n",
    "    time_fr = []\n",
    "\n",
    "    for j in range(0,trialShape-windowBin,mov_10ms):\n",
    "        try:\n",
    "            fr = el.statistics.mean_firing_rate(poiss_spiketrain,j*binwin*pq.s,(j+windowBin)*binwin*pq.s).magnitude\n",
    "\n",
    "        except:\n",
    "            fr = 0 \n",
    "        \n",
    "        time_fr.append(fr)\n",
    "\n",
    "    return time_fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def circdistance(len_my_list, idx_1, idx_2):\n",
    "    i = (idx_1 - idx_2) % len_my_list\n",
    "    j = (idx_2 - idx_1) % len_my_list\n",
    "    return min(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binnedPos(xPos,xTime,yPos,yTime,binwin):\n",
    "    \n",
    "    x = xPos\n",
    "    xtime = xTime-min(xTime)\n",
    "    fx = interpolate.interp1d(xtime,x)\n",
    "    \n",
    "    \n",
    "    y = yPos\n",
    "    ytime = yTime-min(yTime)\n",
    "    fy = interpolate.interp1d(ytime,y)\n",
    "    \n",
    "    newTime =  np.arange(0, max(max(xtime),max(ytime)), binwin)\n",
    "    xnew = fx(newTime)\n",
    "    ynew = fy(newTime)\n",
    "    \n",
    "    return xnew,ynew,newTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time2bin(time,binwin=0.01,buffer=0.45,lastBin=False, window=0.2):\n",
    "    \n",
    "    # the default is to get the first bin of integration, but sometimes we want the last bin! \n",
    "    if lastBin:\n",
    "        bin_idx = int((time+buffer-window)/binwin) \n",
    "    else:\n",
    "        bin_idx = int(((time)+(buffer))/binwin)\n",
    "        \n",
    "    return bin_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin2time(bin_idx,binwin=0.01,buffer=0.45,lastBin=False, window=0.2):\n",
    "        # the default is to get the first bin of integration, but sometimes we want the last bin! \n",
    "    if lastBin:\n",
    "        time = (bin_idx*binwin)-buffer+window\n",
    "    else:\n",
    "        time = (bin_idx*binwin)-buffer\n",
    "    \n",
    "    return time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_percent_hits(neighbors_by_labels):\n",
    "    \n",
    "    percent_hits = []\n",
    "    for row in neighbors_by_labels:\n",
    "        hit_count = 0\n",
    "        label = row[0]\n",
    "        neighbors = row[1:]\n",
    "        for neighbor in neighbors:\n",
    "            if neighbor==label:\n",
    "                hit_count += 1\n",
    "        score = 1.0*hit_count / len(neighbors)\n",
    "        percent_hits.append(score)\n",
    "        \n",
    "    return percent_hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flattenNetwork(FN):\n",
    "    return FN.flatten('F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def thresholdNetwork(FN,percentile,type ='network', returnDim = True, binary=False):\n",
    "#     if FN.ndim == 2 :\n",
    "#         FN_flattened = flattenNetwork(FN)\n",
    "#     elif FN.ndim == 1:\n",
    "#         FN_flattened = FN\n",
    "#     else:\n",
    "#         \"Dimensions of FN not accepted\"\n",
    "#         return None \n",
    "#     if type=='network':\n",
    "#         if binary:\n",
    "#             thresholded_FN = np.where(FN_flattened > np.percentile(FN_flattened,percentile), 1, 0)\n",
    "#         else:\n",
    "#             thresholded_FN = np.where(FN_flattened > np.percentile(FN_flattened,percentile), FN_flattened, 0)\n",
    "    \n",
    "    \n",
    "\n",
    "#     if returnDim:\n",
    "#         thresholded_FN = np.reshape(thresholded_FN,FN.shape)\n",
    "\n",
    "#     return thresholded_FN\n",
    "\n",
    "#threshold Network beta update: has type argument\n",
    "def thresholdNetwork(FN,percentile,type ='network', direction = None, returnDim = True, binary=False):\n",
    "\n",
    "\n",
    "    if type =='network':\n",
    "        # NETWORK-wise thresholding means we are taking the top p percentile of weights of the whole network\n",
    "        # check network dimensions\n",
    "        if FN.ndim == 2 :\n",
    "            FN_flattened = flattenNetwork(FN)\n",
    "        elif FN.ndim == 1:\n",
    "            FN_flattened = FN\n",
    "        else:\n",
    "            \"Dimensions of FN not accepted\"\n",
    "            return None \n",
    "\n",
    "        if binary:\n",
    "            thresholded_FN = np.where(FN_flattened > np.percentile(FN_flattened,percentile), 1, 0)\n",
    "        else:\n",
    "            thresholded_FN = np.where(FN_flattened > np.percentile(FN_flattened,percentile), FN_flattened, 0)\n",
    "\n",
    "    elif type =='node':\n",
    "        # NODE-wise thresholding. We look at either the in- or out- going connections and threshold per node.\n",
    "        # check network dimensions; we are going row by row (source node-wise) so we actually want an adjacency matrix\n",
    "        if FN.ndim == 2 :\n",
    "            square_FN = FN\n",
    "        elif FN.ndim == 1:\n",
    "            square_FN = flat2squareNetwork(FN)\n",
    "        else:\n",
    "            \"Dimensions of FN not accepted\"\n",
    "            return None\n",
    "\n",
    "        # check that argument direction exists (possible values: 'in','out','undirected')\n",
    "        if direction  == 'out' or direction == 'undirected':\n",
    "            # we treat undirected as 'out', since theoretically this should be the same if we used 'in'\n",
    "            if binary:\n",
    "                thresholded_FN = [np.where(x > np.percentile(x,percentile), 1, 0) for x in square_FN]\n",
    "            else:\n",
    "                thresholded_FN = [np.where(x > np.percentile(x,percentile), x, 0) for x in square_FN]\n",
    "\n",
    "        elif direction == 'in':\n",
    "            if binary:\n",
    "                thresholded_FN = [np.where([x[c] for x in square_FN] > np.percentile([x[c] for x in square_FN],percentile), 1, 0)  for c in range(len(square_FN))]\n",
    "            else:\n",
    "                thresholded_FN = [np.where([x[c] for x in square_FN] > np.percentile([x[c] for x in square_FN],percentile), [x[c] for x in square_FN], 0)  for c in range(len(square_FN))]\n",
    "        else: \n",
    "            \"The 'direction' argument is invalid. Please specify the direction you want to threshold ('in'/'out') or if undirection ('undirected')\"\n",
    "            return None\n",
    "\n",
    "    if returnDim:\n",
    "        thresholded_FN = np.reshape(thresholded_FN,FN.shape)\n",
    "\n",
    "    return thresholded_FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadPickle(filepath):\n",
    "    with open(filepath, 'rb') as f:\n",
    "        data = pickle.load(f)   \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def savePickle(filepath,save_obj):\n",
    "    f = open(filepath,\"wb\")\n",
    "    pickle.dump(save_obj,f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDirection(x,y):\n",
    "    # x and y are arrays. we will get the direction of the reach based on the end points\n",
    "    # output is in degrees\n",
    "    direction = np.arctan2(y[-1]-y[0],x[-1]-x[0]) * 180 / np.pi\n",
    "    direction = (direction+360) % 360\n",
    "    return direction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tripletCC(W,weighted=True):\n",
    "    weighted = True\n",
    "    motifs = ['cycle','middleman','fan-in','fan-out']\n",
    "\n",
    "    CC_by_motif = {}\n",
    "    d_o  = np.count_nonzero(W, axis=0)\n",
    "    d_i  = np.count_nonzero(W, axis=1)\n",
    "    d_bi = np.count_nonzero(np.where(np.dot(W,W.transpose())>0,1,0),axis=0) #np.count_nonzero(np.diagonal(np.dot(W,W)))\n",
    "    if weighted:\n",
    "        A = np.cbrt(W)\n",
    "    else:\n",
    "        A = W\n",
    "\n",
    "    for motif in motifs:\n",
    "        CC_by_motif[motif] = {}\n",
    "\n",
    "        if motif =='cycle':\n",
    "            actual_motif_count =  np.diagonal(np.linalg.multi_dot([A,A,A]))\n",
    "            possible_motif_count = ((d_o*d_i)-d_bi)\n",
    "        if motif =='middleman':\n",
    "            actual_motif_count =  np.diagonal(np.linalg.multi_dot([A,A.transpose(),A]))\n",
    "            possible_motif_count = ((d_o*d_i)-d_bi)\n",
    "        if motif =='fan-in':\n",
    "            actual_motif_count =  np.diagonal(np.linalg.multi_dot([A.transpose(),A,A]))\n",
    "            possible_motif_count = d_i*(d_i-1)\n",
    "        if motif =='fan-out':\n",
    "            actual_motif_count =  np.diagonal(np.linalg.multi_dot([A,A,A.transpose()]))\n",
    "            possible_motif_count = d_o*(d_o-1)\n",
    "\n",
    "        CC = actual_motif_count/possible_motif_count\n",
    "        \n",
    "        CC[CC==0] = np.nan\n",
    "        CC = np.nan_to_num(CC,nan=0,posinf=0,neginf=0)\n",
    "        CC_by_motif[motif]['meanCC'] = np.mean(CC)\n",
    "        CC_by_motif[motif]['byNode'] = CC\n",
    "        \n",
    "    return CC_by_motif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LaplacianFlatten(FN):\n",
    "    FN_reshaped = flat2squareNetwork(FN)\n",
    "    G = nx.from_numpy_matrix(FN_reshaped,create_using=nx.DiGraph())\n",
    "    G_L = np.asarray(nx.linalg.directed_laplacian_matrix(G))\n",
    "    G_L_flattened = flattenNetwork(G_L)\n",
    "    return G_L_flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat2squareNetwork(FN):\n",
    "    return np.reshape(FN, (int(np.sqrt(FN.shape[0])),int(np.sqrt(FN.shape[0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jitterSpikes(x,jitter, precision=0.001):\n",
    "    # add +/- range to each element of x\n",
    "    # assumes spiketimes are in seconds and jitters a millisecond\n",
    "    rand_array = np.arange(-jitter,jitter,precision)\n",
    "    jittered_x = np.sort([y+np.random.choice(rand_array) for y in x])\n",
    "    return jittered_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poissSpkTrain_data(T,dt,r_data):\n",
    "\n",
    "    t_steps = np.arange(0,T,dt)\n",
    "\n",
    "    spkTimes = []\n",
    "    isi = []\n",
    "    spkTrain = []\n",
    "    t_old = 0 \n",
    "    k=0\n",
    "    for i in t_steps:\n",
    "        tmp = random.random()\n",
    "        pf = r_data[k]\n",
    "\n",
    "        if tmp <= pf:\n",
    "            spkTimes.append(i)\n",
    "            isi.append(i-t_old)\n",
    "            t_old = i\n",
    "\n",
    "            r = 0\n",
    "\n",
    "            spkTrain.append(1)\n",
    "        else:\n",
    "            spkTrain.append(0)\n",
    "\n",
    "        k +=1\n",
    "    return spkTimes,isi,spkTrain,t_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lscov(A, B, w=None):\n",
    "    \"\"\"Least-squares solution in presence of known covariance\n",
    "\n",
    "    :math:`A \\\\cdot x = B`, that is, :math:`x` minimizes\n",
    "    :math:`(B - A \\\\cdot x)^T \\\\cdot \\\\text{diag}(w) \\\\cdot (B - A \\\\cdot x)`.\n",
    "    The matrix :math:`w` typically contains either counts or inverse\n",
    "    variances.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    A: matrix or 2d ndarray\n",
    "        input matrix\n",
    "    B: vector or 1d ndarray\n",
    "        input vector\n",
    "\n",
    "    Notes\n",
    "    --------\n",
    "    https://de.mathworks.com/help/matlab/ref/lscov.html\n",
    "    \"\"\"\n",
    "    # https://stackoverflow.com/questions/27128688/how-to-use-least-squares-with-weight-matrix-in-python\n",
    "    # https://de.mathworks.com/help/matlab/ref/lscov.html\n",
    "    if w is None:\n",
    "        Aw = A.copy()\n",
    "        Bw = B.T.copy()\n",
    "    else:\n",
    "        W = np.sqrt(np.diag(np.array(w).flatten()))\n",
    "        Aw = np.dot(W, A)\n",
    "        Bw = np.dot(B.T, W)\n",
    "\n",
    "    # set rcond=1e-10 to prevent diverging odd indices in x\n",
    "    # (problem specific to ggf/stress computation)\n",
    "    x, residuals, rank, s = np.linalg.lstsq(Aw, Bw.T, rcond=1e-10)\n",
    "    return np.array(x).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_reciprocity(FN):\n",
    "    w_r = np.minimum(FN,FN.transpose())\n",
    "    w_nr = FN-w_r\n",
    "    r = np.sum(w_r)/np.sum(FN)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeNetworkMetric(data,null):\n",
    "    return (data-null)/(1-null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample FN\n",
    "def subsampleFN(FN,nodes = None, num_nodes=None):\n",
    "    # check that FN is square\n",
    "    if FN.shape[0]!=FN.shape[1]:\n",
    "        print(\"Error: Dimensions of FN not accepted. Must be square (NxN)!\")\n",
    "        return None \n",
    "\n",
    "    if nodes is None and num_nodes is None:\n",
    "        print(\"Error: you must either put a number of nodes (num_nodes) to subsample or an array with node IDs (nodes)!\")\n",
    "        return None\n",
    "    else:\n",
    "        # check that num_nodes<original number of nodes\n",
    "        if num_nodes is not None:\n",
    "            if FN.shape[0]<num_nodes:\n",
    "                print(\"Error: num_nodes must be less than the number of the original nodes in your network!\")\n",
    "                return None \n",
    "            else:\n",
    "                resampled_nodes = np.sort(np.random.choice(np.arange(FN.shape[0]),num_nodes,replace=False))\n",
    "        elif nodes is not None:\n",
    "            if FN.shape[0]<np.max(nodes) and FN.shape[0]<len(nodes) :\n",
    "                print(\"Error: nodes must be less than the number of the original nodes in your network!\")\n",
    "                return None \n",
    "            else:\n",
    "                resampled_nodes = nodes\n",
    "\n",
    "    # get subsampled FN\n",
    "    resampled_FN = np.array([np.array([FN[i,j] for j in resampled_nodes]) for i in resampled_nodes])\n",
    "    \n",
    "    return resampled_FN,resampled_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified from: https://stackoverflow.com/questions/61760669/numpy-1d-array-find-indices-of-boundaries-of-subsequences-of-the-same-number\n",
    "def first_and_last_seq(x, n):\n",
    "    a = np.r_[n-1,x,n-1]\n",
    "    a = a==n\n",
    "    start = np.r_[False,~a[:-1] & a[1:]]\n",
    "    end = np.r_[a[:-1] & ~a[1:], False]\n",
    "    return [np.where(start)[0]-1, np.where(end)[0]-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://stackoverflow.com/a/35094823\n",
    "def autoscale_y(ax,margin=0.1):\n",
    "    \"\"\"This function rescales the y-axis based on the data that is visible given the current xlim of the axis.\n",
    "    ax -- a matplotlib axes object\n",
    "    margin -- the fraction of the total height of the y-data to pad the upper and lower ylims\"\"\"\n",
    "\n",
    "    import numpy as np\n",
    "\n",
    "    def get_bottom_top(line):\n",
    "        xd = line.get_xdata()\n",
    "        yd = line.get_ydata()\n",
    "        lo,hi = ax.get_xlim()\n",
    "        y_displayed = yd[((xd>lo) & (xd<hi))]\n",
    "        h = np.max(y_displayed) - np.min(y_displayed)\n",
    "        bot = np.min(y_displayed)-margin*h\n",
    "        top = np.max(y_displayed)+margin*h\n",
    "        return bot,top\n",
    "\n",
    "    lines = ax.get_lines()\n",
    "    bot,top = np.inf, -np.inf\n",
    "\n",
    "    for line in lines:\n",
    "        new_bot, new_top = get_bottom_top(line)\n",
    "        if new_bot < bot: bot = new_bot\n",
    "        if new_top > top: top = new_top\n",
    "\n",
    "    ax.set_ylim(bot,top)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
