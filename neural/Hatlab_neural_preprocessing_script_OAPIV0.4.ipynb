{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "727afc89",
   "metadata": {},
   "source": [
    "## Spike Processing Pipeline ver 0.2\n",
    "This notebook specifes a preprocessing pipeline for neural data colleted on blackrock aquisition devices in the Hatsopoulos lab. The file assumes that spikeinterface(ver 0.13 - 0.10), python-neo, matplotlib, numpy, PHY, and pandas are installed.\n",
    "\n",
    "The code needs a datafile (ns6) and a probe file (you can find tony's its at /Paul/prbfiles/TY_array.prb). This script will automatically precondition the data (filtering and referencing), attach probe infomation (via .prb file) and pull out LFP data. The file will then sort the neural data with a number of different sorters, and comparitive sort to find the units common to all of the sorters (see https://elifesciences.org/articles/61834 ). Finally, it will convert the automatic sorting of one sorter (It is recommened to use IronClust as your primary sorter) and mark the common units as 'good' in the phy output.\n",
    "\n",
    "The user would then mark each cluster as good (SU), MUA (multiunit), or noise. For analysis, you could read the phy output directly (numpy files) or you could inmport back to spikeinterface and save.\n",
    "\n",
    "Useful web sites:\n",
    "spikeinterface docs (for old api): https://spikeinterface.readthedocs.io/en/0.13.0/\n",
    "phy docs: https://phy.readthedocs.io/en/latest/\n",
    "Useful info about .prb files): https://tridesclous.readthedocs.io/en/latest/important_details.html\n",
    "                               https://spyking-circus.readthedocs.io/en/latest/code/probe.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc88badb",
   "metadata": {},
   "source": [
    "## preprocessing --SESSION NAME--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228d6848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary toolboxes. Note unknown installation problem with matplotlib. Run this cell 2x. Will complain\n",
    "# but ok.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import spikeinterface\n",
    "import spikeinterface.extractors as se \n",
    "import spikeinterface.toolkit as st\n",
    "import spikeinterface.sorters as ss\n",
    "import spikeinterface.comparison as sc\n",
    "import spikeinterface.widgets as sw\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# specify inline plotting\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca94c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User defined information about the data\n",
    "\n",
    "pth = '/media/paul/storage/Data/Tony/' # path to data directory\n",
    "sess = 'TY20210211_inHammock_night/' # directory where Ns6 file lives\n",
    "file = 'TY20210211_inHammock_night-002.ns6' # name of NS6 file\n",
    "prbfile = '/media/paul/storage/Data/TY_array.prb' # name of probe (.prb) file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32a5bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify path to recording\n",
    "recording_folder = pth+sess+file\n",
    "print('recording_folder: ', recording_folder)\n",
    "# load recording\n",
    "recording = se.BlackrockRecordingExtractor(recording_folder, nsx_to_load=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bde7b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load probe information\n",
    "probefile = prbfile\n",
    "print('probefile: ', probefile)\n",
    "recording_prb = recording.load_probe_file(probefile)\n",
    "# check that info correct properties are present (should be gain, group, location, name, and offset)\n",
    "recording_prb.get_shared_channel_property_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db8201f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize probe geometry: want to see that it looks correct\n",
    "w_elec = sw.plot_electrode_geometry(recording_prb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11fb940",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "First step is common median filtering (the median across all channels is removed sample wise from the signal at each probe). Then the signal at each channel is low pass filtered and saved off as the lfp. The data is then bandpass filtered for spike processing. The filter values can be set, the defaults are 350Hz for LFP and 350-7500Hz for spikes. Some plots to review the data and save progress. The spike data is saved as a cache. All of the filtering is only done when the data is read. So to save time in future steps, we save the filtered/processed data to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d5c568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# automagically attempt top find bad channels. need to then specify them in next cell. Don't forget to add 1 \n",
    "# to the channel ID found in this step. Python zero based. amiright?\n",
    "st.preprocessing.remove_bad_channels(recording_prb, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85873bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove bad channels: First time you run this skip.\n",
    "recording_rmc = st.preprocessing.remove_bad_channels(recording_prb, bad_channel_ids=[])\n",
    "# verify that all of the properties were trnasfered to the new recording object\n",
    "print('properties: ', recording_rmc.get_shared_channel_property_names())\n",
    "# verify bad channels have been removed\n",
    "print('ids: ', recording_rmc.get_channel_ids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2527519a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# condition the signal for the lfp\n",
    "# lowpass filter for lfp\n",
    "recording_lp = st.preprocessing.bandpass_filter(recording_prb, freq_min=1, freq_max=350, filter_type='butter')\n",
    "# downsample\n",
    "recording_lfp = st.preprocessing.resample(recording_lp, resample_rate=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b94a4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bandpass filter for spikes\n",
    "recording_f = st.preprocessing.bandpass_filter(recording_rmc, freq_min=350, freq_max=7500, filter_type='butter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a05f716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# common median reference. First time switch input to recording_prb. Make it recording_rmc if you remove channels \n",
    "recording_cmr = st.preprocessing.common_reference(recording_f, reference='median')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fe5fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the signal on channels. channel_id is the probe and trange is the time sample to view in seconds\n",
    "sw.plot_timeseries(recording_cmr, channel_ids=[2, 5, 7], trange=[0, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6d6d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the power spectrum of the data. Check that the filtering looks reasonable. You can also look at the\n",
    "# the raw data: recording, or the lfp: recording_lfp, or the spike data: recording_f\n",
    "w_sp = sw.plot_spectrum(recording_prb, channels=[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890892eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save preprocessed data for spikes and cache recording\n",
    "recording_cache = se.CacheRecordingExtractor(recording_cmr, save_path=pth+sess+'processed/filtered_data.dat')\n",
    "recording_cache.dump_to_dict()\n",
    "recording_cache.dump_to_pickle(pth+sess+'processed/recording.pkl')\n",
    "# save preprocessed data for lfp\n",
    "se.CacheRecordingExtractor(recording_lfp, save_path=pth+sess+'processed/lfp_data.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9f4aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load up from save: If you need to reload the data above, you can just run this cell\n",
    "recording_cache = se.load_extractor_from_pickle(pth+sess+'processed/recording.pkl')\n",
    "# check the channel properties are correct\n",
    "recording_cache.get_shared_channel_property_names()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bba5bb",
   "metadata": {},
   "source": [
    "## Spike Sorting\n",
    "First step is to set the parameters for the sorters. Then call each sorter that you want to run. Each run will make its own folder with the sorting results and save the results. You can also add aditional sorters. For example if you want to add YASS you can follow the pattern set here. See the documentation to install."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13780a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the paths for the sorters you want to run\n",
    "ss.Kilosort2_5Sorter.set_kilosort2_5_path('/media/paul/storage/MLToolBoxes/Kilosort-2.5/')\n",
    "ss.IronClustSorter.set_ironclust_path('/media/paul/storage/MLToolBoxes/ironclust/')\n",
    "ss.WaveClusSorter.set_waveclus_path('/media/paul/storage/MLToolBoxes/wave_clus/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75208eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check which sorters are installed\n",
    "ss.installed_sorters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bb9cde",
   "metadata": {},
   "source": [
    "## Spyking Circus\n",
    "Spyking circus nicely complains when an electrode is too corrupted to sort, so run this first and check to see if there are any problem electrodes. You can find which channels didn't get sorted by searching the results, each probe has its own folder, so search: ls results_sc/?/recording/*result.hdf5.  If so, you can go back and exclude those probes from the analysis by inserting the problematic probe numbers into the previous cell for exluding probes (don't forget that the probes will be in python 0-based, and you need to specify in the cell above the probe number in 1-based system). When bad electrodes are found Spyking Circus will tell you in the log which probe it is, then it will crash just before it completes. You can find the bad probes becuase they will be missing a *recording.result.hdf5* file in that probes recording directory (for ex. for the first probe the file would be located at processed/results_sc/0/recording/). Once you get spyking circus to run through to completion, everything else should work no problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12ea4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with spyking circus. list the parameters for the sorter\n",
    "ss.get_params_description('spykingcircus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6639f39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see what the default parameters are. These are the only parameters spikeinterface will let you modify.\n",
    "ss.get_default_params('spykingcircus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47726c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set your own parameter values\n",
    "params = {'detect_sign': -1,\n",
    " 'adjacency_radius': 100,\n",
    " 'detect_threshold': 6,\n",
    " 'template_width_ms': 3,\n",
    " 'filter': False,\n",
    " 'merge_spikes': True,\n",
    " 'auto_merge': 0.75,\n",
    " 'num_workers': 15,\n",
    " 'whitening_max_elts': 1000,\n",
    " 'clustering_max_elts': 10000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcb8a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the sorter\n",
    "sorting_SC = ss.run_spykingcircus(recording_cache, \n",
    "                                  output_folder=pth+sess+'processed/results_sc', \n",
    "                                  grouping_property='group',\n",
    "                                  n_jobs=5,\n",
    "                                  verbose=True, \n",
    "                                  **params)\n",
    "print(f'SpykingCircus found {len(sorting_SC.get_unit_ids())} units')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acfd720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attempt to save sorting results in case of crash\n",
    "sorting_SC.dump_to_dict()\n",
    "sorting_SC.dump_to_pickle(pth+sess+'processed/sorting_sc.pkl')\n",
    "# to reload\n",
    "#sorting_SC = se.load_extractor_from_pickle('sorting_sc.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64342ebc",
   "metadata": {},
   "source": [
    "## Ironclust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87707407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ironclust tends to do ok for our arrays same procedure view available paramters\n",
    "ss.get_params_description('ironclust')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc842a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter default values\n",
    "ss.get_default_params('ironclust')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c445e636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our own paramter dict\n",
    "params = {'detect_sign': -1,\n",
    " 'adjacency_radius': 50,\n",
    " 'adjacency_radius_out': 100,\n",
    " 'detect_threshold': 3.5,\n",
    " 'prm_template_name': '',\n",
    " 'freq_min': 300,\n",
    " 'freq_max': 8000,\n",
    " 'merge_thresh': 0.7,\n",
    " 'pc_per_chan': 9,\n",
    " 'whiten': False,\n",
    " 'filter_type': 'none',\n",
    " 'filter_detect_type': 'none',\n",
    " 'common_ref_type': 'trimmean',\n",
    " 'batch_sec_drift': 300,\n",
    " 'step_sec_drift': 20,\n",
    " 'knn': 30,\n",
    " 'n_jobs_bin': 1,\n",
    " 'chunk_mb': 500,\n",
    " 'min_count': 30,\n",
    " 'fGpu': True,\n",
    " 'fft_thresh': 8,\n",
    " 'fft_thresh_low': 0,\n",
    " 'nSites_whiten': 16,\n",
    " 'feature_type': 'gpca',\n",
    " 'delta_cut': 1,\n",
    " 'post_merge_mode': 1,\n",
    " 'sort_mode': 1,\n",
    " 'fParfor': True,\n",
    " 'filter': False,\n",
    " 'clip_pre': 0.25,\n",
    " 'clip_post': 0.75,\n",
    " 'merge_thresh_cc': 1,\n",
    " 'nRepeat_merge': 3,\n",
    " 'merge_overlap_thresh': 0.95}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e06dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run sorter\n",
    "sorting_IC = ss.run_ironclust(recording_cache, \n",
    "                              output_folder=pth+sess+'processed/results_ic', \n",
    "                              grouping_property='group', \n",
    "                              n_jobs=5, \n",
    "                              verbose=True,\n",
    "                              **params)\n",
    "print(f'Ironclust found {len(sorting_IC.get_unit_ids())} units')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75fe040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attempt to save sorting results in case of crash\n",
    "sorting_IC.dump_to_dict()\n",
    "sorting_IC.dump_to_pickle(pth+sess+'processed/sorting_ic.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1c9327",
   "metadata": {},
   "source": [
    "## Waveclus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38398633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# waveclus takes a long time. see parameters\n",
    "ss.get_params_description('waveclus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac8186b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default values\n",
    "ss.get_default_params('waveclus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573c932b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify parameters\n",
    "params = {'detect_threshold': 4,\n",
    " 'detect_sign': -1,\n",
    " 'feature_type': 'wav',\n",
    " 'scales': 4,\n",
    " 'min_clus': 40,\n",
    " 'maxtemp': 0.251,\n",
    " 'template_sdnum': 3,\n",
    " 'enable_detect_filter': True,\n",
    " 'enable_sort_filter': True,\n",
    " 'detect_filter_fmin': 300,\n",
    " 'detect_filter_fmax': 3000,\n",
    " 'detect_filter_order': 4,\n",
    " 'sort_filter_fmin': 300,\n",
    " 'sort_filter_fmax': 3000,\n",
    " 'sort_filter_order': 2,\n",
    " 'mintemp': 0,\n",
    " 'w_pre': 20,\n",
    " 'w_post': 44,\n",
    " 'alignment_window': 10,\n",
    " 'stdmax': 50,\n",
    " 'max_spk': 40000,\n",
    " 'ref_ms': 1.5,\n",
    " 'interpolation': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17879764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run sorter\n",
    "sorting_WC = ss.run_waveclus(recording_cache, \n",
    "                             output_folder=pth+sess+'processed/results_wc_04', \n",
    "                             grouping_property='group', \n",
    "                             n_jobs=5, \n",
    "                             verbose=True,\n",
    "                             **params)\n",
    "print(f'Waveclus found {len(sorting_WC.get_unit_ids())} units')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31a63fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attempt to save sorting results in case of crash\n",
    "sorting_WC.dump_to_dict()\n",
    "sorting_WC.dump_to_pickle(pth+sess+'processed/sorting_wc_03.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e528c51",
   "metadata": {},
   "source": [
    "## Klusta Kwik\n",
    "Only use if Tridesclous or kilosort doesn't work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bcddf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the parameters\n",
    "ss.get_params_description('klusta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccc2420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default values\n",
    "ss.get_default_params('klusta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de6ff62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify parameters\n",
    "params = {'adjacency_radius': None,\n",
    " 'threshold_strong_std_factor': 5,\n",
    " 'threshold_weak_std_factor': 2,\n",
    " 'detect_sign': -1,\n",
    " 'extract_s_before': 16,\n",
    " 'extract_s_after': 32,\n",
    " 'n_features_per_channel': 3,\n",
    " 'pca_n_waveforms_max': 10000,\n",
    " 'num_starting_clusters': 50,\n",
    " 'chunk_mb': 500,\n",
    " 'n_jobs_bin': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e247a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run sorter\n",
    "sorting_KL = ss.run_klusta(recording_cache, \n",
    "                           output_folder=pth+sess+'processed/results_kl',\n",
    "                           grouping_property='group',\n",
    "                           n_jobs=5, \n",
    "                           verbose=True, \n",
    "                           **params)\n",
    "print(f'klusta found {len(sorting_KL.get_unit_ids())} units')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566ab3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attempt to save sorting results in case of crash\n",
    "sorting_KL.dump_to_dict()\n",
    "sorting_KL.dump_to_pickle(pth+sess+'processed/sorting_kl.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dd1d76",
   "metadata": {},
   "source": [
    "## Try Kilosort\n",
    "You need a GPU to run kilosort. Additionally you need to compile the cuda mex files. See installation instructions for kilosort. Use ver 2.5 or 2.0. Version 3 has issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2bb13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try kilosort. it usually doesn't work with the marmoset data, but sometimes it does. Only use v2.5 or v2.0. \n",
    "# see params\n",
    "ss.get_params_description('kilosort2_5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9118134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see default vales\n",
    "ss.get_default_params('kilosort2_5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb8733a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set desired parameter values\n",
    "params = {'detect_threshold': 6,\n",
    " 'projection_threshold': [5, 2],\n",
    " 'preclust_threshold': 5,\n",
    " 'car': True,\n",
    " 'minFR': 0.1,\n",
    " 'minfr_goodchannels': 0.1,\n",
    " 'nblocks': 5,\n",
    " 'sig': 20,\n",
    " 'freq_min': 150,\n",
    " 'sigmaMask': 60,\n",
    " 'nPCs': 3,\n",
    " 'ntbuff': 64,\n",
    " 'nfilt_factor': 4,\n",
    " 'NT': None,\n",
    " 'keep_good_only': False,\n",
    " 'chunk_mb': 500,\n",
    " 'n_jobs_bin': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a4b226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run sorter (dont worry if it crashes. Just go to the next cell). note that this sorter does not use the group\n",
    "# parameter. Kilosort assumes electrode conatacts are <20um, so that a spatial signal is aquired. So it wont work\n",
    "# with one channel. Might be able to finagle something with 3 channels, but needs development. \n",
    "sorting_KS = ss.run_kilosort2_5(recording_cache, \n",
    "                                output_folder=pth+sess+'processed/results_ks', \n",
    "                                #grouping_property='group', \n",
    "                                n_jobs=5, \n",
    "                                verbose=True,\n",
    "                                **params)\n",
    "print(f'Kilosort2.5 found {len(sorting_KS.get_unit_ids())} units')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612989a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attempt to save sorting results in case of crash\n",
    "sorting_KS.dump_to_dict()\n",
    "sorting_KS.dump_to_pickle(pth+sess+'processed/sorting_ks.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917d8488",
   "metadata": {},
   "source": [
    "## Try Tridesclous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3389a9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the properties of sorter\n",
    "ss.get_params_description('tridesclous')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96aa724c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see what the default parameters are. These are the only parameters spikeinterface will let you modify.\n",
    "ss.get_default_params('tridesclous')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787dd527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the paramters are dict. Make a new dict with the parameters you want. One thing you generally need to change is\n",
    "# to remove the options for filtering, as the data is already preprocessed. Though sometimes this isn't an option \n",
    "# (well at least that I have figured out how to change)\n",
    "params = {\n",
    "    'freq_min': 400.0,\n",
    "    'freq_max': 5000.0,\n",
    "    'detect_sign': -1,\n",
    "    'detect_threshold': 4,\n",
    "    'peak_span_ms': 0.7,\n",
    "    'wf_left_ms': -2.0,\n",
    "    'wf_right_ms': 3.0,\n",
    "    'feature_method': 'auto',\n",
    "    'cluster_method': 'auto',\n",
    "    'clean_catalogue_gui': False,\n",
    "    'chunk_mb': 500,\n",
    "    'n_jobs_bin': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2a7c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the sorter. Always try and run with the group option (this sorts each probe individually, which is what we\n",
    "# want for blackrock arrays since each probe is ~400um away)\n",
    "sorting_TDC = ss.run_tridesclous(recording_cache, \n",
    "                                 output_folder=pth+sess+'processed/results_tdc', \n",
    "                                 grouping_property='group', \n",
    "                                 n_jobs=5, \n",
    "                                 verbose=True,\n",
    "                                 **params)\n",
    "print(f'Tridesclous found {len(sorting_TDC.get_unit_ids())} units')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33eee0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attempt to save sorting results in case of crash\n",
    "sorting_TDC.dump_to_dict()\n",
    "sorting_TDC.dump_to_pickle(pth+sess+'processed/sorting_tdc.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8383bb31",
   "metadata": {},
   "source": [
    "## Curating the spike sorting\n",
    "Curation means to check the output of the spike sorter and decide if it was accurate. This can genrally be done in three ways. Criteria: set thresholds for metrics and reject clusters that don't meet the threshold. For example: ISI violations, SNR, Distance from noise clusters, etc. Comparitive: This tries to find the same unit bewteen each sorter in a pairwise fashion. it then identifies units that are common to all of the sorters. According to the paper (see ref at top), four sorters provided the optimal information and performed well with little manual intervention. Manual: Examining each cluster to decide if it's a single unit or not. \n",
    "\n",
    "Here we take a combined approach of comapritive and criteria curation combined with manual validation. This allows the user to have the ultimate say and provides the oppurtunity to save multiunit clusters. There is no guarentee that clusters identified by all of the sorters is a single unit. There could be errors (the cluster is really garbage), splits (two of the clusters actually belong in the same cluster, or one cluster is aactually two cells), or some combintation (an identifed cluster is actually a seperable noise and unit cluster)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e07e0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare sorter outputs. Its important here to have your primary sorter as the first sorter. The primary sorter \n",
    "# is the sort that you will actually process and use. I currently recommend using Ironclust with the default\n",
    "# parameters as your primary sorters. The following code reflects this decision. If you want to use a different\n",
    "# sorter as primary, you need to adjust the code below\n",
    "\n",
    "mcmp = sc.compare_multiple_sorters([sorting_IC, sorting_TDC, sorting_KS, sorting_SC], ['IC', 'TDC', 'KS', 'SC'], \n",
    "                                   spiketrain_mode='union', n_jobs=1, \n",
    "                                   verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0722ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize comaprisons\n",
    "w = sw.plot_multicomp_agreement(mcmp)\n",
    "w = sw.plot_multicomp_agreement_by_sorter(mcmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6d2c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set agreement sorter. min agreement count is the number of sorters that had to agree to count the unit. Default is\n",
    "# is to use 4 (as per the paper)\n",
    "agreement_sorting = mcmp.get_agreement_sorting(minimum_agreement_count=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119e3587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the ids of the common units\n",
    "ids = agreement_sorting.get_unit_ids()\n",
    "# show user common unit ids from the primary sorter\n",
    "print('Common Unit IDs: ', ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05fd65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crucial: cache main sorter and specify location of tmp directory. The tmp directory needs to exist in your system.\n",
    "# This will eat a lot of space while its processing. I would recomment that you have 2TB+ free in the tmp directory\n",
    "sorting_IC_cache = se.CacheSortingExtractor(sorting_IC, pth+sess+'processed/ic_sort_results_cache.dat')\n",
    "sorting_IC_cache.dump_to_pickle(pth+sess+'processed/ic_sorting_cache.pkl')\n",
    "sorting_IC_cache.set_tmp_folder(pth+sess+'processed/tmp/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db91d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the data to phy for manual curation. This should work, but if it crashes, you will need to do the \n",
    "# processing separately. Use the code cells at the end of this docuiment to do that, than rerun this cell.\n",
    "st.postprocessing.export_to_phy(recording_cache, sorting_IC_cache, \n",
    "                                output_folder=pth+sess+'processed/phy_IC', \n",
    "                                ms_before=0.5, \n",
    "                                ms_after=1, \n",
    "                                compute_pc_features=True, \n",
    "                                compute_amplitudes=True, \n",
    "                                max_spikes_per_unit=None, \n",
    "                                compute_property_from_recording=False, \n",
    "                                n_jobs=1, \n",
    "                                recompute_info=False, \n",
    "                                save_property_or_features=False, \n",
    "                                verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9ac57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quality metrics\n",
    "import seaborn as sns\n",
    "# get quality metrics\n",
    "quality_metrics = st.validation.compute_quality_metrics(sorting_IC, recording_cache, \n",
    "                                                        metric_names=['firing_rate', 'isi_violation', 'snr'], \n",
    "                                                        as_dataframe=True)\n",
    "# plot the data\n",
    "plt.figure()\n",
    "# you can change these however you want to see the values\n",
    "sns.scatterplot(data=quality_metrics, x=\"snr\", y='isi_violation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3972d174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide thresholds for quality metrics and ID sites that pass criteria\n",
    "snr_thresh = 5\n",
    "isi_viol_thresh = 0.5\n",
    "# first get ISI violations and see ids that pass\n",
    "sorting_auto = st.curation.threshold_isi_violations(sorting_IC, isi_viol_thresh, 'greater', duration)\n",
    "print('#: ', len(sorting_auto.get_unit_ids()))\n",
    "print('IDs: ', sorting_auto.get_unit_ids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715e609a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now threshold on snr, and additionally remove clusters that do not pass\n",
    "sorting_auto = st.curation.threshold_snrs(sorting_auto, recording_cache, snr_thresh, 'less')\n",
    "print('#: ', len(sorting_auto.get_unit_ids()))\n",
    "print('IDs: ', sorting_auto.get_unit_ids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ef6511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto label based on criteria and comparision analysis. We do that by labelling all clusters that passed our\n",
    "# criteria as MUA. Then we go back and label all clusters that were found in all sorters as 'Good'(SU).\n",
    "cfile = pth+sess+'processed/phy_IC/cluster_group.tsv'\n",
    "cg = pd.read_csv(cfile, delimiter='\\t')\n",
    "cg.iloc[sorting_auto.get_unit_ids(), 1] = 'mua'\n",
    "cg.iloc[ids, 1] = 'good'\n",
    "cg.to_csv(cfile, index=False, sep='\\t') # check to see if the correct units were marked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec4beb6",
   "metadata": {},
   "source": [
    "### Done!\n",
    "You should now use Phy to manually curate the results. I recommend having Phy installed in a seprate environment. When you open Phy, all of the clusters should be labeled \"mua\" (gray), \"good\" (green), or unlabeled (white). They can also be labeled \"noise\" (dark gray). You can now go through and adjust the automated curation. You want to check that good units are indeed single units to you, and that none of the mua are actually single units. You also want to check for errors (which should be rare) like: A cluster needs to be split into two. Two clusters need to be mereged. The cluster is really some weird noise. A cluster acually has labeled the same events as another cluster. Remeber to save your work in phy through the menu often."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3976da0",
   "metadata": {},
   "source": [
    "### Saving the output/working with the data\n",
    "\n",
    "After completing the manual curation we can access the spike info directly from phy using python (phy output is in .npy) or matlab (using the numpy toolbox). Or you can reimport the data form phy and save as nwb or access directy (the spikeinterface format is really neo under the hood). The NWB part is still a work in progress and may be problematic. The method attempted here is to save the processed data. then append the sorted data to the NWB file created for the processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f801284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import phy curation\n",
    "sorting_TDC_phy = se.PhySortingExtractor('/media/paul/storage/Data/Theseus/phy_TDC/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f067e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to NWB. Allegedly this exports the data to NWB format. It doesn't like it if there is already a file that \n",
    "# has the name of your output file. (so delete it if it already exists)\n",
    "outputfile = pth+sess+'SPLTest01.nwb'\n",
    "se.NwbRecordingExtractor.write_recording(recording_cache, outputfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efea2a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the sorting data to the NWB file by using setting the overwrite argument to False\n",
    "se.NwbSortingExtractor.write_sorting(sorting_TDC_phy, outputfile, overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812db221",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4dbba840",
   "metadata": {},
   "source": [
    "# Extra cells that might be helpful"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f023b6c",
   "metadata": {},
   "source": [
    "If export to phy crashes, try running these next three cells first, then retry export to phy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da6d142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get waveforms for chosen sorter\n",
    "st.postprocessing.get_unit_waveforms(recording_cache, sorting_IC_cache, \n",
    "                                     ms_before=0.5, \n",
    "                                     ms_after=1, \n",
    "                                     compute_property_from_recording=True, \n",
    "                                     n_jobs=10, \n",
    "                                     max_spikes_per_unit=None, \n",
    "                                     memmap=True, \n",
    "                                     save_property_or_features=True, \n",
    "                                     recompute_info=True, \n",
    "                                     verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ba83f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get amplitudes for chosen sorter\n",
    "st.postprocessing.get_unit_amplitudes(recording_cache, sorting_IC_cache, \n",
    "                                      ms_before=0.5, \n",
    "                                      ms_after=1, \n",
    "                                      max_spikes_per_unit=None, \n",
    "                                      memmap=True, \n",
    "                                      save_property_or_features=True, \n",
    "                                      n_jobs=10, \n",
    "                                      verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c361c3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unit templates\n",
    "st.postprocessing.get_unit_templates(recording_cache, sorting_IC_cache, \n",
    "                                     ms_before=0.5, \n",
    "                                     ms_after=1, \n",
    "                                     max_spikes_per_unit=None, \n",
    "                                     memmap=True, \n",
    "                                     save_property_or_feature=True, \n",
    "                                     n_jobs=10, \n",
    "                                     verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e684525",
   "metadata": {},
   "source": [
    "In case the isi violations throws an error, try running these cells first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393eb7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "duration = recording_cache.get_num_frames()\n",
    "isi_violations = st.validation.compute_isi_violations(sorting_IC, duration_in_frames=duration)\n",
    "print('ISI violations:', isi_violations)\n",
    "\n",
    "snrs = st.validation.compute_snrs(sorting_IC, recording_cache)\n",
    "print('SNRs:', snrs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
